<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AI on Notes from the digital underground by Lngnmn</title>
    <link>https://lngnmn2.github.io/categories/ai/</link>
    <description>Recent content in AI on Notes from the digital underground by Lngnmn</description>
    <generator>Hugo -- 0.147.7</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 04 Oct 2025 11:41:48 +0545</lastBuildDate>
    <atom:link href="https://lngnmn2.github.io/categories/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLMs and AI so far</title>
      <link>https://lngnmn2.github.io/articles/llms-and-ai-so-far/</link>
      <pubDate>Sat, 04 Oct 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/llms-and-ai-so-far/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s summarize the current state of Large Language Models (LLMs) and so called &amp;ldquo;Artificial Intelligence&amp;rdquo; (AI) as of October 2025.&lt;/p&gt;
&lt;p&gt;They all are still just [estimated] probabilities of the next token, given the &amp;ldquo;context&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;This implies no &amp;ldquo;knowledge&amp;rdquo; or &amp;ldquo;understanding&amp;rdquo; [of any kind] whatsoever. Nothing can be taken as &amp;ldquo;true&amp;rdquo; or even &amp;ldquo;correct&amp;rdquo; or &amp;ldquo;accurate&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;All the talks about &amp;ldquo;knowledge in the weights&amp;rdquo; or &amp;ldquo;knowledge encoded within the network&amp;rdquo; is just bullshit.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Probabilistic bullshit</title>
      <link>https://lngnmn2.github.io/articles/probabilistic-bullshit/</link>
      <pubDate>Tue, 30 Sep 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/probabilistic-bullshit/</guid>
      <description>&lt;p&gt;Look, ma, a new episode just dropped! This one is full of shit to the brim. Even more so than prof.  &lt;em&gt;Ellen Langer&lt;/em&gt; who cannot stay within a context and claimed that 1+1 = 10 &lt;em&gt;because&lt;/em&gt; in the &lt;em&gt;binary notation it looks like 10 in decimal&lt;/em&gt;&amp;hellip; anyway, whatever.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=MlmFj1-mOtg&#34;&gt;https://www.youtube.com/watch?v=MlmFj1-mOtg&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;No, the brain &lt;em&gt;ain&amp;rsquo;t computing  any hecking probabilities&lt;/em&gt;. It is not a Bayesian machine. It is not a prediction machine. It is not a simulator. It is not a statistical engine.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Prompt engineers, lmao</title>
      <link>https://lngnmn2.github.io/articles/prompt-engineers-lmao/</link>
      <pubDate>Fri, 26 Sep 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/prompt-engineers-lmao/</guid>
      <description>&lt;p&gt;Time waits for no one, the race to the bottom accelerates faster than ever, and the &amp;ldquo;future&amp;rdquo; is now. Competition is severe and mostly meaningless, as in some third-world criminal infested ghetto. This is what LLMs turned our world into.&lt;/p&gt;
&lt;p&gt;So, lets &amp;ldquo;pee on&amp;rdquo; so called &amp;ldquo;prompt engineers&amp;rdquo;, in the &lt;code&gt;4chan&lt;/code&gt; parlance, of course..&lt;/p&gt;
&lt;p&gt;Here is my benchmark prompt to evaluate performance of LLMs. All the &amp;ldquo;simple&amp;rdquo; offline models fail miserably, and only Grok and Gemini can produce something adequate.  Claude is also good, but it is supposed to be the best, being trained especially for code generation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fuck you, gemini</title>
      <link>https://lngnmn2.github.io/articles/fuck-you-gemini/</link>
      <pubDate>Thu, 25 Sep 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/fuck-you-gemini/</guid>
      <description>&lt;p&gt;FUCK THIS SHIT!&lt;/p&gt;
&lt;p&gt;No, really. It just hallucinated a non-existent package, with such a confidence lmao&lt;/p&gt;
&lt;p&gt;Yes, yes, I know, it cannot know anything about &amp;ldquo;existence&amp;rdquo;, but at least these dorks do not &amp;ldquo;deserve&amp;rdquo; the exuberant money they got paid.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Uniform Code Block Rendering in Eww and Shr
To get syntax highlighting in eww and shr, you need to use a package that intercepts &amp;lt;code&amp;gt; and &amp;lt;pre&amp;gt; tags and applies Emacs&amp;rsquo;s built-in font-locking. The eww-code-blocks-mode is a good choice for this.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DeepMind and OpenAI win Gold</title>
      <link>https://lngnmn2.github.io/articles/deepmind-and-openai-win-gold/</link>
      <pubDate>Thu, 18 Sep 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/deepmind-and-openai-win-gold/</guid>
      <description>Look, ma, another day another bullshit.</description>
    </item>
    <item>
      <title>Let me show you something</title>
      <link>https://lngnmn2.github.io/articles/let-me-show-you-something/</link>
      <pubDate>Tue, 16 Sep 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/let-me-show-you-something/</guid>
      <description>To see things as they [realy] are</description>
    </item>
    <item>
      <title>Defeating Nondeterminism, my ass</title>
      <link>https://lngnmn2.github.io/articles/defeating-nondeterminism/</link>
      <pubDate>Thu, 11 Sep 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/defeating-nondeterminism/</guid>
      <description>LOL, LMAO even</description>
    </item>
    <item>
      <title>the LLM upanishad</title>
      <link>https://lngnmn2.github.io/articles/the-llm-upanishad/</link>
      <pubDate>Thu, 11 Sep 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/the-llm-upanishad/</guid>
      <description>The modern Eastern philosophy</description>
    </item>
    <item>
      <title>LLM (and Math) Philosophy 202</title>
      <link>https://lngnmn2.github.io/articles/llm-and-math-phil-202/</link>
      <pubDate>Mon, 08 Sep 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/llm-and-math-phil-202/</guid>
      <description>&lt;p&gt;DESCRIPTION: Idiots, idiots everywhere.jpg&lt;/p&gt;
&lt;p&gt;I am getting old and my cognitive abilities are slowly declining, and I will have no pension or social security, so I need a &amp;ldquo;Turing Award&amp;rdquo; or something. (only partially kidding).  And, yes, I &amp;ldquo;hacked&amp;rdquo; these &amp;ldquo;longevity&amp;rdquo; memes and &amp;ldquo;protocols&amp;rdquo;, but seems like fighting an increase in entropy and the second law of thermodynamics is not that efficient as some meme-guys claim on YouTube ).&lt;/p&gt;
&lt;p&gt;So, lets settle one fundamental meme-question once and for all, so we can let it all go and focus on &amp;ldquo;bio-hacking&amp;rdquo; and what not.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Humanties in 30 seconds</title>
      <link>https://lngnmn2.github.io/articles/humanties-in-30-seconds/</link>
      <pubDate>Mon, 25 Aug 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/humanties-in-30-seconds/</guid>
      <description>&lt;p&gt;So-called &lt;em&gt;Humanties&lt;/em&gt; (as the way of  producing a sectarian socially constructed pretentious abstract verbiage) are done for. At least none of them deserve that high upper-middle-class social status anymore.&lt;/p&gt;
&lt;p&gt;Yes, you have to be &lt;em&gt;actually smart&lt;/em&gt; to ask the &amp;ldquo;right questions&amp;rdquo; (be a prompt &lt;em&gt;engineers&lt;/em&gt; LMAO!), but still. Watch how it spews out an outstanding &amp;ldquo;paper&amp;rdquo; in 30 seconds.&lt;/p&gt;
&lt;p&gt;I really don&amp;rsquo;t know what I have studied (wasted my life) for. Yes, yes, these are not &lt;em&gt;final&lt;/em&gt; answers, and it missed subtleties, and more accurate, insightful formulations, but who needs them anymore?&lt;/p&gt;</description>
    </item>
    <item>
      <title>It Feels Like A Cheating Because It Is</title>
      <link>https://lngnmn2.github.io/articles/it-feels-like-cheating-because-it-is/</link>
      <pubDate>Mon, 11 Aug 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/it-feels-like-cheating-because-it-is/</guid>
      <description>&lt;p&gt;The purpose of examination in a college or university setting is to empirically verify that a student has &lt;em&gt;his own genuine understanding&lt;/em&gt; and could actually apply it. It is that simple.&lt;/p&gt;
&lt;p&gt;This is why some courses even allow to bring a textbook to an exam, because if one does not [already] understand the principles and techniques, the textbook is of no use.&lt;/p&gt;
&lt;p&gt;However, bringing pocket calculators, smartphones is considered a cheating, precisely because by using these devises one could &lt;em&gt;appear&lt;/em&gt; as possessing  one&amp;rsquo;s own understanding.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Programming Before AI</title>
      <link>https://lngnmn2.github.io/articles/programming-before-ai/</link>
      <pubDate>Mon, 11 Aug 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/programming-before-ai/</guid>
      <description>&lt;p&gt;I have to rush to finish my series before AI will completely enshittificate the written knowledge and the classic discipline of programming, which is based on understanding and mathematical rigour.&lt;/p&gt;
&lt;p&gt;Yes, I seemingly overuse this word beyond the limits of a good style, but I like it, how it is awkward and ugly and yet perfectly captures what a generative AI is doing to a writtern knowledge itself.&lt;/p&gt;
&lt;p&gt;So, let&amp;rsquo;s try to build it all top-down, for a change. Bottom up is probably the better way, but it takes so long time before one begins to see the whole picture, which is, of course, the one and the same mountain (or an elephant) viewed from different angles and perspectives.&lt;/p&gt;</description>
    </item>
    <item>
      <title>openai-gpt-oss-20b</title>
      <link>https://lngnmn2.github.io/articles/openai-gpt-oss-20b/</link>
      <pubDate>Wed, 06 Aug 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/openai-gpt-oss-20b/</guid>
      <description>&lt;p&gt;OpenAI has recently benedicted us with a 20 billion parameter &amp;ldquo;open source&amp;rdquo; model, which is a significant step up from the previous 7 billion parameter model. This model is designed to be more efficient and effective in understanding and generating human-like text, or rather to &lt;em&gt;appear&lt;/em&gt; to do so.&lt;/p&gt;
&lt;p&gt;It is a total crap, by the way, at least compared to the online free-tier GROK (which is also very basic and limited). It is, obviously, &amp;ldquo;competing&amp;rdquo; with the DeepSeek&amp;rsquo;s &amp;ldquo;open source&amp;rdquo; offerings, and have a very similar &amp;ldquo;feel&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Let the bubble burst, for Crist&#39;s sake!</title>
      <link>https://lngnmn2.github.io/articles/let-the-bubble-burst/</link>
      <pubDate>Sun, 03 Aug 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/let-the-bubble-burst/</guid>
      <description>Look, ma, no errors!</description>
    </item>
    <item>
      <title>The Knowledge Work Bubble</title>
      <link>https://lngnmn2.github.io/articles/the-knowledge-work-bubble/</link>
      <pubDate>Sat, 02 Aug 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/the-knowledge-work-bubble/</guid>
      <description>  The end of the &amp;#34;knowledge work&amp;#34; as we know it.
  </description>
    </item>
    <item>
      <title>Look ma, 100x engineers</title>
      <link>https://lngnmn2.github.io/articles/100x-engineering/</link>
      <pubDate>Wed, 23 Jul 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/100x-engineering/</guid>
      <description>Another day -- another bullshit</description>
    </item>
    <item>
      <title>Gold medal-level performance at IMO.</title>
      <link>https://lngnmn2.github.io/articles/openai-gold-medal/</link>
      <pubDate>Sun, 20 Jul 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/openai-gold-medal/</guid>
      <description>Memes, memes everywhere.</description>
    </item>
    <item>
      <title>Grok4 launch video</title>
      <link>https://lngnmn2.github.io/articles/grok4-launch-video/</link>
      <pubDate>Thu, 10 Jul 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/grok4-launch-video/</guid>
      <description>&lt;p&gt;So I watched some. The launch video I mean. Closed the frame when they began that &amp;ldquo;voice&amp;rdquo; thing.&lt;/p&gt;
&lt;p&gt;The PhD. level across all subjects is just a meme. I can easily be shown by asking the question that require a beyond memorizing textbooks reasoning.&lt;/p&gt;
&lt;p&gt;I don&amp;rsquo;t want to &amp;ldquo;register&amp;rdquo; for &amp;ldquo;Grok4&amp;rdquo;, but here are some example problems which will break the &amp;ldquo;PhD level&amp;rdquo; meme.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A recursive functions in, say, Ocaml, or  Scala, without explicitly mentioning the accumulator pattern as the required way to avoid stack overflows on languages which does not do TCO at the compile time. This is  very basic stuff, which all &amp;ldquo;PhDs&amp;rdquo; have to know. The inner &lt;em&gt;lambda&lt;/em&gt; with an extra argument, and the  &amp;ldquo;trampoline&amp;rdquo;  is such a classic pattern that some compliers do it automatically. Again, &lt;em&gt;without mentioning it&lt;/em&gt; the model will fail by writing non TCO code .&lt;/p&gt;</description>
    </item>
    <item>
      <title>Illusion Of Intelligence</title>
      <link>https://lngnmn2.github.io/articles/illusion-of-intelligence/</link>
      <pubDate>Sat, 05 Jul 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/illusion-of-intelligence/</guid>
      <description>&lt;p&gt;There is a very simple trick to  break the illusion and to see through &amp;ldquo;the veil of Maya&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Locally-run models, like &lt;em&gt;Deepseek-R1-14b-0528&lt;/em&gt; (at full &lt;code&gt;fp16&lt;/code&gt; quantization),  which is the best I could have,  produce vastly different &amp;ldquo;answers&amp;rdquo; for exactly the same prompt not just between two runs, but if one uses a different math library stack (like recompiling with forcing Intel MKL).&lt;/p&gt;
&lt;p&gt;Every  time  we run a prompt a reasonably good model  spits out something which &amp;ldquo;looks very reasonable&amp;rdquo;, (unless your are an actual expert in the field), because it captures  &amp;ldquo;common sense&amp;rdquo;, expressed in the training data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Software In The Era of AI</title>
      <link>https://lngnmn2.github.io/articles/software-intheeraof-ai/</link>
      <pubDate>Thu, 19 Jun 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/software-intheeraof-ai/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LCEmiRjPEtQ&#34;&gt;https://www.youtube.com/watch?v=LCEmiRjPEtQ&lt;/a&gt; and, of course, the No.1 spot on the Chuddie safe space  &lt;a href=&#34;https://news.ycombinator.com/item?id=44314423&#34;&gt;https://news.ycombinator.com/item?id=44314423&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Karpathy is shilling &amp;ldquo;Cursor&amp;rdquo; and other cloud-based mettered AI services (which have to pay back their debts). Probably has an interest in it and some other meme AI startups. Nothing to see here.&lt;/p&gt;
&lt;p&gt;We should some day know which marketing &amp;ldquo;genious&amp;rdquo; came up with this &amp;ldquo;winning strategy&amp;rdquo; &amp;ndash; to metter every single token (byte) and try to sell this to corporations. Corporations do not want to be mettered like that, they want to metter normies, the way Cellular operators do, and they never use any normies plans themselves.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yes, it is time to scream and  panic</title>
      <link>https://lngnmn2.github.io/articles/the-very-serious-post/</link>
      <pubDate>Thu, 12 Jun 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/the-very-serious-post/</guid>
      <description>Sometimes not getting what you what is itself a biggest strike of luck.</description>
    </item>
    <item>
      <title>Enshittification Of Knowledge</title>
      <link>https://lngnmn2.github.io/articles/enshittification-of-knowledge/</link>
      <pubDate>Tue, 03 Jun 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/enshittification-of-knowledge/</guid>
      <description>&lt;p&gt;There are some philosophical &amp;ldquo;ideals&amp;rdquo;, which has been identified since antiquity and to attainment (or approaching of) which people are striving ever since.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To see things as they really are.&lt;/li&gt;
&lt;li&gt;To do things just right way.&lt;/li&gt;
&lt;li&gt;To find an optimum or a &amp;ldquo;perfection&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Perfection has been famously defined as &amp;ldquo;when there is nothing else (more) to take away (to remove)&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Modern meme-based socially-constructed (by retarded majority) social concensus frown upon &amp;ldquo;perfectionism&amp;rdquo; and sees it as the inhibition to &amp;ldquo;getting shit done&amp;rdquo;. They are not wrong, though.  Approaching a perfection (finding a local optimum) is a very different process from just putting together some slop. Yes, indeed, &amp;ldquo;perfection is the enemy of good-enough&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bullshit Bullshit Everywhere</title>
      <link>https://lngnmn2.github.io/articles/bullshit-bullshit-everywhere/</link>
      <pubDate>Sat, 31 May 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/bullshit-bullshit-everywhere/</guid>
      <description>&lt;p&gt;&amp;ldquo;The Darwin Gödel Machine: AI that improves itself by rewriting its own code&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sakana.ai/dgm/&#34;&gt;https://sakana.ai/dgm/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There is what is actually going on. A model trained on a large amount code is, in principle, no different from any other LLMs &amp;ndash; it is just a statistical model that predicts the next token based on the previous ones. It does not understand the code it spews out, it does not &amp;ldquo;know&amp;rdquo; what it is doing. These are just mathematical procedures (not even functions) &amp;ndash; given an input encoded in a particular way, it produces an output, not even the same for the same input.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Carmack On Ai</title>
      <link>https://lngnmn2.github.io/articles/carmack-on-ai/</link>
      <pubDate>Sat, 24 May 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/carmack-on-ai/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://twitter.com/ID_AA_Carmack/status/1925710474366034326&#34;&gt;https://twitter.com/ID_AA_Carmack/status/1925710474366034326&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I have read the notes. they are a mess.&lt;/p&gt;
&lt;p&gt;For me, Carmack, aside from being a legend, is sort of Goggins of &lt;em&gt;imperative procedural programming&lt;/em&gt;, who learned everything by doing without studying the theories first.&lt;/p&gt;
&lt;p&gt;His ultimate strength is, it seems, in a &lt;em&gt;focused doing&lt;/em&gt;, ploughing through a problem, if you will, without being exceedingly dramatic.&lt;/p&gt;
&lt;p&gt;Learning from experience (actual trails and errors and quick feedback loops) and gradual improvement of his own &amp;ldquo;emergent&amp;rdquo; intuitive understanding &amp;ndash; ones own mental model of how things should be done.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reasoning Models Don&#39;t Always Say What They  Think</title>
      <link>https://lngnmn2.github.io/articles/models-dont-say-think/</link>
      <pubDate>Fri, 04 Apr 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/models-dont-say-think/</guid>
      <description>Antrophic, please</description>
    </item>
    <item>
      <title>Fuck This Shit</title>
      <link>https://lngnmn2.github.io/articles/anthropic-tracing-thoughts/</link>
      <pubDate>Fri, 28 Mar 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/anthropic-tracing-thoughts/</guid>
      <description>bullshit, bullshit, bullshit...</description>
    </item>
    <item>
      <title>Large Ladyboy Models</title>
      <link>https://lngnmn2.github.io/articles/large-ladyboy-models/</link>
      <pubDate>Sun, 09 Mar 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/large-ladyboy-models/</guid>
      <description>&lt;p&gt;Classy Andrej is making shilling videos from Thailand (he leaked his location in the video ) targeting normies (the previous set of videos has been partially filmed in Japan. Andrej is living a truly digital nomad&amp;rsquo;s life).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=EWvNQjAaOHw&#34;&gt;https://www.youtube.com/watch?v=EWvNQjAaOHw&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Why would he shill? Well, he and guys like him made a lot of promises, not to us (who tf cares), but to the money guys, that this particular technology will completely transform the world, and that &lt;em&gt;they&lt;/em&gt; are the very top guys in the field, so money shall be given to them (to the affiliated companies and entities).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Coding with LLMs</title>
      <link>https://lngnmn2.github.io/articles/llm-coding/</link>
      <pubDate>Mon, 24 Feb 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/llm-coding/</guid>
      <description>&lt;p&gt;DESCRIPTION: Idiots, idiots everywhere.&lt;/p&gt;
&lt;p&gt;Now I can accurately summarize what coding using LLMs &lt;em&gt;actually /is&lt;/em&gt; in just a few sentences.&lt;/p&gt;
&lt;p&gt;Recall how people usually describe a code maintenance job: &lt;em&gt;we have this code to run, while the original developers are gone and leave us no design documentation&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This hypothetical situation is exactly what you get when an LLM finished spewing out the slop: you now have some code, very cheap, even for free, but it is &lt;em&gt;not yours&lt;/em&gt;, the underlying understanding (of the whys) &lt;em&gt;is not in your head&lt;/em&gt;, and the original developer is already gone. Disappeared.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Grok3</title>
      <link>https://lngnmn2.github.io/articles/grok3/</link>
      <pubDate>Tue, 18 Feb 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/grok3/</guid>
      <description>Smoke and memes.</description>
    </item>
    <item>
      <title>AI Slop</title>
      <link>https://lngnmn2.github.io/articles/ai-slop/</link>
      <pubDate>Sun, 16 Feb 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/ai-slop/</guid>
      <description>&lt;p&gt;&lt;strong&gt;slop&lt;/strong&gt; &lt;em&gt;noun&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;cambridge-dictionary&#34;&gt;Cambridge dictionary&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;food that is more liquid than it should be and is therefore unpleasant&lt;/li&gt;
&lt;li&gt;liquid or wet food waste, especially when it is fed to animals&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;oxford-learner-s-dictionary&#34;&gt;Oxford Learner&amp;rsquo;s Dictionary&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;​waste food, sometimes fed to animals&lt;/li&gt;
&lt;li&gt;liquid or partly liquid waste, for example urine or dirty water from baths&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There is also a very related term &amp;ldquo;&lt;em&gt;goyslop&lt;/em&gt;&amp;rdquo; from internet sewers (losers are always looking for someone to blame and hate [instead of themselves]).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deepseek In Action</title>
      <link>https://lngnmn2.github.io/articles/deepseek-in-action/</link>
      <pubDate>Sat, 15 Feb 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/deepseek-in-action/</guid>
      <description>Look, ma, no reasoning.</description>
    </item>
    <item>
      <title>Reasoning LLMs</title>
      <link>https://lngnmn2.github.io/articles/reasoning-llms/</link>
      <pubDate>Tue, 11 Feb 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/reasoning-llms/</guid>
      <description>&lt;p&gt;AUTHOR: &amp;lt;lngnmn2@yahoo.com&amp;gt;&lt;/p&gt;
&lt;p&gt;When I was a kid &lt;del&gt;they told me not to stare at the sun&lt;/del&gt; I had this vision, that the brain structures are sort of like trees, while the &amp;ldquo;branches&amp;rdquo; are just like patches thorough our yard after fresh snow.&lt;/p&gt;
&lt;p&gt;Some of them remain thin, just someone walked across it absentmindedly, some gets broadened by a heavy re-use. Who would plow through a fresh snow while one could take follow the path that is already here.&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenAI vs. Deepseek</title>
      <link>https://lngnmn2.github.io/articles/openai-vs-deepseek/</link>
      <pubDate>Thu, 30 Jan 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/openai-vs-deepseek/</guid>
      <description>When shit reached the fan.</description>
    </item>
    <item>
      <title>Deepseek R1</title>
      <link>https://lngnmn2.github.io/articles/deepseek-r1/</link>
      <pubDate>Sun, 26 Jan 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/deepseek-r1/</guid>
      <description>&lt;p&gt;DESCRIPTION: Memes and mirrors.&lt;/p&gt;
&lt;p&gt;Nowadays things are moving way too fast. It is not just controlled trial-and-error, it is literally throwing everything at the wall (to see what sticks).&lt;/p&gt;
&lt;p&gt;It started with that meme &amp;ldquo;Attention Is All You Need&amp;rdquo;, when they just came up with an &amp;ldquo;architecture&amp;rdquo; that sticks.&lt;/p&gt;
&lt;p&gt;That &amp;ldquo;attention&amp;rdquo; and &amp;ldquo;multi head attention&amp;rdquo; turned out to be just a few additional layers of a particular kind.&lt;/p&gt;
&lt;p&gt;No one can explain the actual mechanisms of how exactly or even &lt;em&gt;why&lt;/em&gt; the layers are as they are (abstract bullshit aside). While the general idea was to mimic some specialized brain centers (the key to understand how it works), the actual code was merely &amp;ldquo;buffers&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLMs und AI</title>
      <link>https://lngnmn2.github.io/articles/llms-und-ai/</link>
      <pubDate>Wed, 20 Nov 2024 16:53:58 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/llms-und-ai/</guid>
      <description>&lt;p&gt;DATE: &lt;span class=&#34;timestamp-wrapper&#34;&gt;&lt;span class=&#34;timestamp&#34;&gt;&amp;lt;2024-11-20 Wed&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Lets write a few paragraphs which will destroy the current LLM narrative (naive bullshit), while neither any single piece  nor the whole article can be refuted.&lt;/p&gt;
&lt;p&gt;This is a high-level proper (classic Eastern) philosophy, which is many levels away from simple logical forms, but it still can be reduced to these, if one wants to. The Proper Philosophy isn&amp;rsquo;t dead, not even it is dying. It cannot, lmao.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Attention Is All bullshit.</title>
      <link>https://lngnmn2.github.io/articles/attention-is-all-bullshit/</link>
      <pubDate>Tue, 04 Jun 2024 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/attention-is-all-bullshit/</guid>
      <description>To see things as they really are.</description>
    </item>
    <item>
      <title>LLM Philosophy 101</title>
      <link>https://lngnmn2.github.io/articles/llm-phil-101/</link>
      <pubDate>Tue, 21 May 2024 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/llm-phil-101/</guid>
      <description>Bullshit, bullshit, bullshit... (K-PAX)</description>
    </item>
    <item>
      <title>Late to the party.</title>
      <link>https://lngnmn2.github.io/articles/late/</link>
      <pubDate>Mon, 27 Nov 2023 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/late/</guid>
      <description>&lt;p&gt;This is sort of an answer to this question.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=38425475&#34;&gt;https://news.ycombinator.com/item?id=38425475&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, what if you are late to the party?&lt;/p&gt;
&lt;p&gt;Unfortunately, nowadays it is even much harder to get through all the utter bullshit and hype, but there is a sort of a shortcut or &amp;ldquo;the Hard Way&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;There are two and a half key figures: &lt;em&gt;Geoffrey Hinton&lt;/em&gt;, who did most of the mathematical heavy lifting, &lt;em&gt;Andrew Ng&lt;/em&gt;, who not just did all the derivations, but became the most famous practitioner, and &lt;em&gt;Andrej Karpathy&lt;/em&gt; who us just a narcissistic asshole, similar to Lex Friedman.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Selfawareness</title>
      <link>https://lngnmn2.github.io/articles/selfawareness/</link>
      <pubDate>Mon, 20 Nov 2023 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/selfawareness/</guid>
      <description>Abstract bullshitting is not enough.</description>
    </item>
    <item>
      <title>A small step towards GAI</title>
      <link>https://lngnmn2.github.io/articles/look-ma-no-i/</link>
      <pubDate>Sat, 21 Oct 2023 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/look-ma-no-i/</guid>
      <description>&lt;p&gt;DESCRIPTION: There is no &amp;ldquo;I&amp;rdquo; in your AI.&lt;/p&gt;
&lt;p&gt;Lets put all the memes and bullshit aside, for a moment and talk serously about GAI (hello, mr. Carmack, sir).&lt;/p&gt;
&lt;p&gt;There is what every &amp;ldquo;AI researcher&amp;rdquo; should know about Knowledge and
Intelligence (yes, both capitalized).&lt;/p&gt;
&lt;p&gt;There &lt;em&gt;is&lt;/em&gt; so-called &amp;ldquo;reality&amp;rdquo; &lt;em&gt;prior to&lt;/em&gt; any knowledge or intelligence. Any
reasonable thinker has been arrived at the ultimate reality.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;I am That&amp;rdquo;&lt;/em&gt; (&amp;ldquo;That Thou Art&amp;rdquo;) of Chandogya Upanishad is the &amp;ldquo;end of
knowledge&amp;rdquo; and the &amp;ldquo;arrival&amp;rdquo; to the ultimate &amp;ldquo;truth&amp;rdquo;, with implies
existence of &amp;ldquo;That&amp;rdquo; and one being just a sub-process (a wave) in It.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLMS For Coding</title>
      <link>https://lngnmn2.github.io/articles/llms-for-coding/</link>
      <pubDate>Sat, 26 Aug 2023 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/llms-for-coding/</guid>
      <description>&lt;p&gt;Today &lt;a href=&#34;https://news.ycombinator.com/&#34;&gt;https://news.ycombinator.com/&lt;/a&gt; is glowing bright with AI memes and buzzwords like a Christmas tree. Everyone is there, including billion dollar corporations announcing a &amp;ldquo;CodeLama-34b&amp;rdquo; which is &lt;em&gt;&amp;ldquo;designed for general code synthesis and understanding.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First of all, I personaly do not want to rely in any part of my life on any &amp;ldquo;synthesized&amp;rdquo; (and &amp;ldquo;understood&amp;rdquo; software, and demand an explicit opt-out. Yes, yes, I know.&lt;/p&gt;
&lt;p&gt;If I have any understanding of these subjects at all, this is a bubble and &lt;em&gt;irrational exuberance&lt;/em&gt;. Lets try to unpack &amp;ldquo;the whys&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GNU Emacs</title>
      <link>https://lngnmn2.github.io/articles/llm-predictions/</link>
      <pubDate>Fri, 28 Jul 2023 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/llm-predictions/</guid>
      <description>  The monument and a world-heritage &amp;#34;site&amp;#34;.
  </description>
    </item>
  </channel>
</rss>
