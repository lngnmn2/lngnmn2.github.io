<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AI on Notes from the digital underground by Lngnmn</title>
    <link>https://lngnmn2.github.io/tags/ai/</link>
    <description>Recent content in AI on Notes from the digital underground by Lngnmn</description>
    <generator>Hugo -- 0.131.0</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Jan 2025 08:24:56 +0545</lastBuildDate>
    <atom:link href="https://lngnmn2.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deepseek R1</title>
      <link>https://lngnmn2.github.io/articles/deepseek-r1/</link>
      <pubDate>Sun, 26 Jan 2025 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/deepseek-r1/</guid>
      <description>DESCRIPTION: Memes and mirrors.
Nowadays things are moving way too fast. It is not just controlled trial-and-error, it is literally throwing everything at the wall (to see what sticks).
It started with that meme &amp;ldquo;Attention Is All You Need&amp;rdquo;, when they just came up with an &amp;ldquo;architecture&amp;rdquo; that sticks.
That &amp;ldquo;attention&amp;rdquo; and &amp;ldquo;multi head attention&amp;rdquo; turned out to be just a few additional layers of a particular kind.
No one can explain the actual mechanisms of how exactly or even why the layers are as they are (abstract bullshit aside).</description>
    </item>
    <item>
      <title>LLMs und AI</title>
      <link>https://lngnmn2.github.io/articles/llms-und-ai/</link>
      <pubDate>Wed, 20 Nov 2024 16:53:58 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/llms-und-ai/</guid>
      <description>DATE: &amp;lt;2024-11-20 Wed&amp;gt;
Lets write a few paragraphs which will destroy the current LLM narrative (naive bullshit), while neither any single piece nor the whole article can be refuted.
This is a high-level proper (classic Eastern) philosophy, which is many levels away from simple logical forms, but it still can be reduced to these, if one wants to. The Proper Philosophy isn&amp;rsquo;t dead, not even it is dying. It cannot, lmao.</description>
    </item>
    <item>
      <title>Attention Is All bullshit.</title>
      <link>https://lngnmn2.github.io/articles/attention-is-all-bullshit/</link>
      <pubDate>Tue, 04 Jun 2024 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/attention-is-all-bullshit/</guid>
      <description>To see things as they really are.</description>
    </item>
    <item>
      <title>LLM Philosophy 101</title>
      <link>https://lngnmn2.github.io/articles/llm-phil-101/</link>
      <pubDate>Tue, 21 May 2024 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/llm-phil-101/</guid>
      <description>Bullshit, bullshit, bullshit... (K-PAX)</description>
    </item>
    <item>
      <title>LLM predictions</title>
      <link>https://lngnmn2.github.io/articles/llm-predictions/</link>
      <pubDate>Wed, 08 Nov 2023 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/llm-predictions/</guid>
      <description>Modern alchemy and astrology using Linea Algebra on Nvidia GPUs</description>
    </item>
    <item>
      <title>Transformers bullshit everywhere</title>
      <link>https://lngnmn2.github.io/articles/transformer-bullshit/</link>
      <pubDate>Fri, 06 Oct 2023 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/transformer-bullshit/</guid>
      <description>No, there is no hyper-sheres in the Universe, sorry, Chuds.</description>
    </item>
    <item>
      <title>LLMS For Coding</title>
      <link>https://lngnmn2.github.io/articles/llms-for-coding/</link>
      <pubDate>Sat, 26 Aug 2023 00:00:00 +0545</pubDate>
      <guid>https://lngnmn2.github.io/articles/llms-for-coding/</guid>
      <description>Today https://news.ycombinator.com/ is glowing bright with AI memes and buzzwords like a Christmas tree. Everyone is there, including billion dollar corporations announcing a &amp;ldquo;CodeLama-34b&amp;rdquo; which is &amp;ldquo;designed for general code synthesis and understanding.&amp;rdquo;
First of all, I personaly do not want to rely in any part of my life on any &amp;ldquo;synthesized&amp;rdquo; (and &amp;ldquo;understood&amp;rdquo; software, and demand an explicit opt-out. Yes, yes, I know.
If I have any understanding of these subjects at all, this is a bubble and irrational exuberance.</description>
    </item>
  </channel>
</rss>
