<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>LLMs und AI | Notes from the digital underground by Lngnmn</title>
<meta name="keywords" content="AI, deep-learning">
<meta name="description" content="DATE: &lt;2024-11-20 Wed&gt;
Lets write a few paragraphs which will destroy the current LLM narrative (naive bullshit), while neither any single piece nor the whole article can be refuted.
This is a high-level proper (classic Eastern) philosophy, which is many levels away from simple logical forms, but it still can be reduced to these, if one wants to. The Proper Philosophy isn&rsquo;t dead, not even it is dying. It cannot, lmao.">
<meta name="author" content="&amp;lt;lngnmn2@yahoo.com&amp;gt;">
<link rel="canonical" href="https://lngnmn2.github.io/articles/llms-und-ai/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lngnmn2.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lngnmn2.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lngnmn2.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lngnmn2.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://lngnmn2.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lngnmn2.github.io/articles/llms-und-ai/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:wght@300&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@300&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif:wght@300&display=swap" rel="stylesheet">
<style>
font-family: 'Noto Serif', serif;
font-family: 'Noto Sans', sans-serif;
font-family: 'Source Code Pro', monospace;
</style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  

<meta property="og:title" content="LLMs und AI" />
<meta property="og:description" content="DATE: &lt;2024-11-20 Wed&gt;
Lets write a few paragraphs which will destroy the current LLM narrative (naive bullshit), while neither any single piece nor the whole article can be refuted.
This is a high-level proper (classic Eastern) philosophy, which is many levels away from simple logical forms, but it still can be reduced to these, if one wants to. The Proper Philosophy isn&rsquo;t dead, not even it is dying. It cannot, lmao." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lngnmn2.github.io/articles/llms-und-ai/" /><meta property="article:section" content="articles" />
<meta property="article:published_time" content="2024-11-20T16:53:58+05:45" />
<meta property="article:modified_time" content="2024-11-20T16:53:58+05:45" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="LLMs und AI"/>
<meta name="twitter:description" content="DATE: &lt;2024-11-20 Wed&gt;
Lets write a few paragraphs which will destroy the current LLM narrative (naive bullshit), while neither any single piece nor the whole article can be refuted.
This is a high-level proper (classic Eastern) philosophy, which is many levels away from simple logical forms, but it still can be reduced to these, if one wants to. The Proper Philosophy isn&rsquo;t dead, not even it is dying. It cannot, lmao."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Articles",
      "item": "https://lngnmn2.github.io/articles/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LLMs und AI",
      "item": "https://lngnmn2.github.io/articles/llms-und-ai/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLMs und AI",
  "name": "LLMs und AI",
  "description": "DATE: \u0026lt;2024-11-20 Wed\u0026gt;\nLets write a few paragraphs which will destroy the current LLM narrative (naive bullshit), while neither any single piece nor the whole article can be refuted.\nThis is a high-level proper (classic Eastern) philosophy, which is many levels away from simple logical forms, but it still can be reduced to these, if one wants to. The Proper Philosophy isn\u0026rsquo;t dead, not even it is dying. It cannot, lmao.",
  "keywords": [
    "AI", "deep-learning"
  ],
  "articleBody": "DATE: \u003c2024-11-20 Wed\u003e\nLets write a few paragraphs which will destroy the current LLM narrative (naive bullshit), while neither any single piece nor the whole article can be refuted.\nThis is a high-level proper (classic Eastern) philosophy, which is many levels away from simple logical forms, but it still can be reduced to these, if one wants to. The Proper Philosophy isn’t dead, not even it is dying. It cannot, lmao.\nJust like mathematical truth has to be traced back to the axioms [of a particular system] and then constructed (built-up) from these first principles so anyone could validate the soundness (correctness) of every single step, we will trace everything back to What Is, and then build the arguments (and even a few principles).\nSimilarly, just like the only way to validate a scientific fact is to trace it back to an appropriate level of What Is – Systems Biology, say, or Molecular Biology, or Organic Chemistry, Physical Chemistry or all the way back to the particular properties of individual atoms., ions or isotopes, and then replicate all the experiments all the way up, as the only [and only one] possible true validation, we will move back and forth in our reasoning.\nThe most fundamental question “how do we know” is already partially answered in the above two paragraphs. We don’t need whole volumes of abstract bullshit. It is always this tracing back to the aspects of “What Is” and then re-building from there all the way up.\nOne more fundamental principle is that there must be no contradictions between what is already known (proved or has beenverified experimentally). Logicians and mathematicians knew that contradictions must not exist, and good physicists worth their name knew it too - since Universe is a process it cannot give rise to contradictions within itself. Even ancient Upanishadic seers intuitively realized this.\nJust in pacing, the famous “Dijkstra argument” about how do we know that an “adder” is functioning correctly is not by trying all the possible operations, of course, but by knowing how it has been designed and implemented, and why. Whys are the pathes (on a causality graph) back to the underlying What Is.\nWhy are all the current LLMs are generating convincingly looking bullshit and that there is no “I” in the current “AI”? Precisely because the underlying code (algorithms) do not preform any tracing backs and build-ups and merely do information processing (something similar to a topological sorting, if you will) at the level of tokens. This information processing cannot, in principle, be a source of “knowledge”, leave alone of a “truth”.\nNo amount of wishfull thinking, memes and nagging will change this fact – the “knowledge” comes from a few levels before the tokens (which are just a chopped off human languages).\nLanguage, which is supposed to (and being used by our very first humans) to describe observed aspects of What Is, comes after What Is.\nAny information processing of any amount of written (or even spoken) language would, again, in principle, be a source of truth or even actual, infallible knowledge. Any reader knows this. We would get religious dogmas, popular abstract doctrines, sectarian bullshit, common sense of the wast majority, and memes upon memes, but not the actual “whys”, and the actual “hows”.\nEven so called “written scientific knowledge” is full of shit (because the writers are biased, and a human language is ambiguous and precise writing is hard).\nLastly, the actual algorithms used for generation are based on “learned estimated probabilities”, and use “randomness” to avoid overfitting (memoization verbatim). This makes it behave like a parrot (the bird) which does exactly the same – makes pseudo-random sounds which are close to what it heard before, without any understanding whatsoever.\nWorking at the level of tokens is precisely the same as the bird “works” at the level of sounds. Read this twice.\nHow do I know? Well, I have seen the code, I have studied the mathematical theories and the underlying principles, I, indeed, traces it back to the What Is and followed the paths of how it has been built in math and code. No intelligence whatsoever. No way to have any knowledge, just like from a parrot.\nYes, both of them could accidentally produce sequences of symbols and sounds which are “approximately correct”, but this means nothing. It is just an imitation, mimicking or a cosplay of intelligence.\nThis very text, however, unlike anything generated, is based on actually going back and forth, which is difficult, time-consuming and costly, so almost no one actually does this.\nThese is a principle from myself, similar to the Dijkstra arguments, but way more general and fundamental:\nThe principle of an abstraction barrier – there is no way, in principle, to know an actual wiring of a processor from the level of the code it runs. Never. It cannot. Just accept. A few layers of abstractions between the code and the wiring are real and impenetrable in principle.\nThis principle shows the existence of a fundamental limit to a human intellect and to any kind of scientific knowledge. Again, ancient Upanishadic seers have intuitively realized this too.\nIf all this is a simulation we could not know. And all your LLMs are jokes.\n",
  "wordCount" : "874",
  "inLanguage": "en",
  "datePublished": "2024-11-20T16:53:58+05:45",
  "dateModified": "2024-11-20T16:53:58+05:45",
  "author":[{
    "@type": "Person",
    "name": "\u0026lt;lngnmn2@yahoo.com\u0026gt;"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lngnmn2.github.io/articles/llms-und-ai/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Notes from the digital underground by Lngnmn",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lngnmn2.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lngnmn2.github.io/" accesskey="h" title="Notes from the digital underground by Lngnmn (Alt + H)">Notes from the digital underground by Lngnmn</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lngnmn2.github.io/about/" title="About me.">
                    <span>About me.</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      LLMs und AI
    </h1>
    <div class="post-meta"><span title='2024-11-20 16:53:58 +0545 +0545'>November 20, 2024</span>&nbsp;·&nbsp;&amp;lt;lngnmn2@yahoo.com&amp;gt;

</div>
  </header> 
  <div class="post-content"><p>DATE: <span class="timestamp-wrapper"><span class="timestamp">&lt;2024-11-20 Wed&gt;</span></span></p>
<p>Lets write a few paragraphs which will destroy the current LLM narrative (naive bullshit), while neither any single piece  nor the whole article can be refuted.</p>
<p>This is a high-level proper (classic Eastern) philosophy, which is many levels away from simple logical forms, but it still can be reduced to these, if one wants to. The Proper Philosophy isn&rsquo;t dead, not even it is dying. It cannot, lmao.</p>
<p>Just like mathematical truth has to be traced back to the axioms [of a particular system] and then constructed (built-up) from these first principles so anyone  could validate the soundness (correctness) of every single step, we will trace everything back to <em>What Is</em>, and then build the arguments (and even a few principles).</p>
<p>Similarly, just like the only way to validate a scientific fact is to trace it back to an appropriate level of <em>What Is</em> &ndash; Systems Biology, say, or Molecular Biology, or Organic Chemistry, Physical Chemistry or all the way back to the particular properties of individual atoms., ions or isotopes, <em>and then</em> replicate all the experiments all the way up, as the only [and only one] possible true validation, we will move back and forth in our reasoning.</p>
<p>The most fundamental question &ldquo;how do we know&rdquo; is already partially answered in the above two paragraphs. We don&rsquo;t need whole volumes of abstract bullshit. It is <em>always</em> this tracing back to the aspects of  &ldquo;<em>What Is</em>&rdquo; and then re-building from there all the way up.</p>
<p>One more fundamental principle is that there must be no contradictions between what is already known (proved or  has beenverified experimentally). Logicians and mathematicians knew that contradictions must not exist, and good physicists worth their name knew it too - since Universe is a process it cannot give rise to contradictions within itself. Even ancient Upanishadic seers intuitively realized this.</p>
<p>Just in pacing, the famous &ldquo;Dijkstra argument&rdquo; about how do we know that an &ldquo;adder&rdquo; is functioning correctly is not by trying all the possible operations, of course, but by knowing how it has been designed and implemented, and <em>why</em>. <em>Whys</em> are the pathes (on a causality graph) back to the underlying <em>What Is</em>.</p>
<p>Why are <em>all</em> the current LLMs are generating convincingly looking bullshit and that there is no &ldquo;I&rdquo; in the current &ldquo;AI&rdquo;? Precisely because the underlying code (algorithms) do not preform any tracing backs and build-ups and merely do <em>information processing (something similar to a topological sorting, if you will) at the level of tokens</em>. This information processing cannot, in principle, be a source of &ldquo;knowledge&rdquo;, leave alone of a &ldquo;truth&rdquo;.</p>
<p>No amount of wishfull thinking, memes and nagging will change this <em>fact</em> &ndash; the &ldquo;knowledge&rdquo; comes from a few levels before the tokens (which are just a chopped off human languages).</p>
<p>Language, which is supposed to (and being used by our very first humans) to describe observed aspects of <em>What Is</em>, comes <em>after What Is</em>.</p>
<p>Any information processing of any amount of written  (or even spoken) language would, again, in principle, be a source of truth or even actual, infallible knowledge. Any reader knows this. We would get religious dogmas, popular abstract doctrines, sectarian bullshit, common sense of the wast majority, and memes upon memes, but not the actual <em>&ldquo;whys&rdquo;</em>, and the actual &ldquo;hows&rdquo;.</p>
<p>Even so called &ldquo;written scientific knowledge&rdquo; is full of shit (because the writers are biased, and  a human language is ambiguous and precise writing is hard).</p>
<p>Lastly, the actual algorithms used for generation are based on &ldquo;learned estimated probabilities&rdquo;, and use &ldquo;randomness&rdquo; to avoid overfitting (memoization verbatim). This makes it behave like a parrot (the bird) which does exactly the same &ndash; makes pseudo-random sounds which are close to what it heard before, without any understanding whatsoever.</p>
<p>Working at the level of tokens is precisely the same as the bird &ldquo;works&rdquo; at the level of sounds. Read this twice.</p>
<p>How do <em>I</em> know? Well, I have seen the code, I have studied the mathematical theories and the underlying principles, I, indeed, traces it back to the <em>What Is</em> and followed the paths of  how it has been built in math and code. No intelligence whatsoever. No way to have any knowledge, just like from a parrot.</p>
<p>Yes, both of them could accidentally produce sequences of symbols and sounds which are &ldquo;approximately correct&rdquo;, but this means nothing. It is just an <em>imitation</em>, mimicking or a <em>cosplay</em> of intelligence.</p>
<p>This very text, however, unlike anything generated, is based on actually going back and forth, which is difficult, time-consuming and costly, so almost no one actually does this.</p>
<p>These is a principle from myself, similar to the Dijkstra arguments, but way more general and fundamental:</p>
<p>The principle of an abstraction barrier &ndash; there is no way, in principle, to know an actual wiring of a processor from the level of the code it runs. Never. It cannot. Just accept. A <em>few</em> layers of abstractions between the code and the wiring are <em>real</em> and <em>impenetrable</em> in principle.</p>
<p>This principle shows the existence of a fundamental limit to a human intellect and to any kind of scientific knowledge. Again, ancient Upanishadic seers have intuitively realized this too.</p>
<p>If <em>all this</em> is a simulation we could not know. And all your LLMs are jokes.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lngnmn2.github.io/tags/ai/">AI</a></li>
      <li><a href="https://lngnmn2.github.io/tags/deep-learning/">Deep Learning</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://lngnmn2.github.io/">Notes from the digital underground by Lngnmn</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
