<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Illusion Of Intelligence | Notes from the digital underground by Lngnmn</title>
<meta name="keywords" content="LLM">
<meta name="description" content="There is a very simple trick to  break the illusion and to see through &ldquo;the veil of Maya&rdquo;.
Locally-run models, like Deepseek-R1-14b-0528 (at full fp16 quantization),  which is the best I could have,  produce vastly different &ldquo;answers&rdquo; for exactly the same prompt not just between two runs, but if one uses a different math library stack (like recompiling with forcing Intel MKL).
Every  time  we run a prompt a reasonably good model  spits out something which &ldquo;looks very reasonable&rdquo;, (unless your are an actual expert in the field), because it captures  &ldquo;common sense&rdquo;, expressed in the training data.">
<meta name="author" content="&amp;lt;lngnmn2@yahoo.com&amp;gt;">
<link rel="canonical" href="https://lngnmn2.github.io/articles/illusion-of-intelligence/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a5781257de4197b8e3d54346acdf1dd55a9ba4cb91dcace192fc1baf228c08b5.css" integrity="sha256-pXgSV95Bl7jj1UNGrN8d1VqbpMuR3KzhkvwbryKMCLU=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lngnmn2.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lngnmn2.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lngnmn2.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lngnmn2.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://lngnmn2.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lngnmn2.github.io/articles/illusion-of-intelligence/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:wght@300&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@300&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif:wght@300&display=swap" rel="stylesheet">
<style>
font-family: 'Noto Serif', serif;
font-family: 'Noto Sans', sans-serif;
font-family: 'Source Code Pro', monospace;
</style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:url" content="https://lngnmn2.github.io/articles/illusion-of-intelligence/">
  <meta property="og:site_name" content="Notes from the digital underground by Lngnmn">
  <meta property="og:title" content="Illusion Of Intelligence">
  <meta property="og:description" content="There is a very simple trick to break the illusion and to see through “the veil of Maya”.
Locally-run models, like Deepseek-R1-14b-0528 (at full fp16 quantization), which is the best I could have, produce vastly different “answers” for exactly the same prompt not just between two runs, but if one uses a different math library stack (like recompiling with forcing Intel MKL).
Every time we run a prompt a reasonably good model spits out something which “looks very reasonable”, (unless your are an actual expert in the field), because it captures “common sense”, expressed in the training data.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="articles">
    <meta property="article:published_time" content="2025-07-05T00:00:00+05:45">
    <meta property="article:modified_time" content="2025-07-05T19:20:43+05:45">
    <meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Illusion Of Intelligence">
<meta name="twitter:description" content="There is a very simple trick to  break the illusion and to see through &ldquo;the veil of Maya&rdquo;.
Locally-run models, like Deepseek-R1-14b-0528 (at full fp16 quantization),  which is the best I could have,  produce vastly different &ldquo;answers&rdquo; for exactly the same prompt not just between two runs, but if one uses a different math library stack (like recompiling with forcing Intel MKL).
Every  time  we run a prompt a reasonably good model  spits out something which &ldquo;looks very reasonable&rdquo;, (unless your are an actual expert in the field), because it captures  &ldquo;common sense&rdquo;, expressed in the training data.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Articles",
      "item": "https://lngnmn2.github.io/articles/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Illusion Of Intelligence",
      "item": "https://lngnmn2.github.io/articles/illusion-of-intelligence/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Illusion Of Intelligence",
  "name": "Illusion Of Intelligence",
  "description": "There is a very simple trick to break the illusion and to see through \u0026ldquo;the veil of Maya\u0026rdquo;.\nLocally-run models, like Deepseek-R1-14b-0528 (at full fp16 quantization), which is the best I could have, produce vastly different \u0026ldquo;answers\u0026rdquo; for exactly the same prompt not just between two runs, but if one uses a different math library stack (like recompiling with forcing Intel MKL).\nEvery time we run a prompt a reasonably good model spits out something which \u0026ldquo;looks very reasonable\u0026rdquo;, (unless your are an actual expert in the field), because it captures \u0026ldquo;common sense\u0026rdquo;, expressed in the training data.\n",
  "keywords": [
    "LLM"
  ],
  "articleBody": "There is a very simple trick to break the illusion and to see through “the veil of Maya”.\nLocally-run models, like Deepseek-R1-14b-0528 (at full fp16 quantization), which is the best I could have, produce vastly different “answers” for exactly the same prompt not just between two runs, but if one uses a different math library stack (like recompiling with forcing Intel MKL).\nEvery time we run a prompt a reasonably good model spits out something which “looks very reasonable”, (unless your are an actual expert in the field), because it captures “common sense”, expressed in the training data.\nIf you run the prompt several times, you will realize that the overall output is a “word-salad” more-or-less about the subject. No “right answer” emerge as again-and-again, it is just different kind of verbiage vomit.\nThe clever online LLM providers employed a nice trick – they assign a unique id (via uuid4) to the pair of prompt and the first generated answer, and when you ask the same prompt again, or reload your browser it will spit out the same crap which, again, looks absolutely convincing, except small nuances, like salmon in the Mediterranean, or olive groves of Okinawa.\nEverything is, literally, an illusion, the Maya, but it is not created out your own confused mind (trained on a wrong crap), but carefully engineered by some of the best brains of the world (for very big money).\nSo, just try to run some prompt that you understand very well, several times on the same model, and you will see that this is just a a stream of verbiage, literal blah-blah-blah, as it usually is in non-exact “sciences” and liberal arts “education”.\nThe situation is very different with a source code on big online models, like Grok3 or Gemini. The most used languages (abundant in the training data), Python in particular, tend to produce comprehensible result and they tend to vary only in some insignificant details (when you know what your are asking for).\nNice languages, less represented in the training sets, perform way worse, for obvious reasons (working in tokens at only the level of syntax, in principle and by the algorithms used). So the code is almost always broken in more than one way, but fixable by a competent person.\nBut non-math and non-coding prompts are just totally broken. You can see it for yourself. Each answer will be subtle different, emphasizing different aspects, and even subtle contradictory to one another.\nThe very best models will be just more subtle wrong, but again, it will be completely different non-deterministic answer, just reminisce about the prompt.\n",
  "wordCount" : "434",
  "inLanguage": "en",
  "datePublished": "2025-07-05T00:00:00+05:45",
  "dateModified": "2025-07-05T19:20:43+05:45",
  "author":[{
    "@type": "Person",
    "name": "\u0026lt;lngnmn2@yahoo.com\u0026gt;"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lngnmn2.github.io/articles/illusion-of-intelligence/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Notes from the digital underground by Lngnmn",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lngnmn2.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lngnmn2.github.io/" accesskey="h" title="Notes from the digital underground by Lngnmn (Alt + H)">Notes from the digital underground by Lngnmn</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lngnmn2.github.io/about/" title="About me.">
                    <span>About me.</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Illusion Of Intelligence
    </h1>
    <div class="post-meta"><span title='2025-07-05 00:00:00 +0545 +0545'>July 5, 2025</span>&nbsp;·&nbsp;<span>&amp;lt;lngnmn2@yahoo.com&amp;gt;</span>

</div>
  </header> 
  <div class="post-content"><p>There is a very simple trick to  break the illusion and to see through &ldquo;the veil of Maya&rdquo;.</p>
<p>Locally-run models, like <em>Deepseek-R1-14b-0528</em> (at full <code>fp16</code> quantization),  which is the best I could have,  produce vastly different &ldquo;answers&rdquo; for exactly the same prompt not just between two runs, but if one uses a different math library stack (like recompiling with forcing Intel MKL).</p>
<p>Every  time  we run a prompt a reasonably good model  spits out something which &ldquo;looks very reasonable&rdquo;, (unless your are an actual expert in the field), because it captures  &ldquo;common sense&rdquo;, expressed in the training data.</p>
<p>If you run the prompt several times, you will realize that the overall  output is a  &ldquo;word-salad&rdquo; more-or-less about the subject. No &ldquo;right answer&rdquo; emerge as again-and-again, it is just different kind of verbiage vomit.</p>
<p>The clever online LLM providers  employed a nice trick &ndash; they assign a unique id (via <code>uuid4</code>) to the <em>pair</em> of prompt and the first generated answer, and when you ask the same prompt again, or reload your browser it will spit out the same crap which, again, looks absolutely convincing, except small nuances, like salmon in the Mediterranean, or olive  groves of Okinawa.</p>
<p>Everything is, literally, an illusion, the Maya, but it is not created out your own confused mind (trained on a wrong crap), but carefully engineered by some of the best brains of the world (for very big money).</p>
<p>So, just try to run some prompt that you understand very well, several times on the same model, and you will see that this is just a a stream of verbiage, literal blah-blah-blah, as it usually is in non-exact &ldquo;sciences&rdquo; and liberal arts &ldquo;education&rdquo;.</p>
<p>The situation is very different with a source code on big online models, like Grok3 or Gemini. The most used languages (abundant in the training data), Python in particular, tend to produce comprehensible result and they tend to vary only in some insignificant details (when you know what your are asking for).</p>
<p>Nice languages, less represented in the training sets, perform way worse, for obvious reasons (working in tokens at only the level of syntax, in principle and by the algorithms used). So the code is almost always broken in more than one way, but fixable by a competent person.</p>
<p>But non-math and non-coding  prompts are just totally broken. You can see it for yourself. Each answer will be subtle different, emphasizing different aspects, and even subtle contradictory to one another.</p>
<p>The very best models will be just more <em>subtle wrong</em>, but again, it will be completely different non-deterministic answer, just reminisce about the prompt.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lngnmn2.github.io/tags/llm/">LLM</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://lngnmn2.github.io/">Notes from the digital underground by Lngnmn</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
